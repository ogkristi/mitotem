{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "from config.settings import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from ray import tune\n",
    "from ray.air import Checkpoint, session\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from src.models.networks import UNet\n",
    "from src.data.loaders import load_data_mitosemseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unet(config, num_workers=0, max_epochs=100):\n",
    "    net = UNet(encoder_depth=config['encoder_depth'])\n",
    "\n",
    "    device = torch.device('cpu')\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=config['lr'], momentum=0.9)\n",
    "\n",
    "    checkpoint = session.get_checkpoint()\n",
    "    if checkpoint:\n",
    "        checkpoint_state = checkpoint.to_dict()\n",
    "        start_epoch = checkpoint_state[\"epoch\"]\n",
    "        net.load_state_dict(checkpoint_state[\"net_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n",
    "    else:\n",
    "        start_epoch = 1\n",
    "\n",
    "    train_iter, val_iter = load_data_mitosemseg(batch_size=config['batch_size'],\n",
    "                                                num_workers=num_workers,\n",
    "                                                split=0.85)\n",
    "\n",
    "    for epoch in range(start_epoch, max_epochs+1):\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, (inputs, targets) in enumerate(train_iter,1):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps +=1\n",
    "            if i % 2000 == 0:\n",
    "                print(f\"[{epoch}, {i:>5}] loss: {running_loss/epoch_steps:.3f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, (inputs, targets) in enumerate(val_iter, 1):\n",
    "            with torch.no_grad():\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                ouputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        checkpoint_data = {\n",
    "            \"epoch\": epoch,\n",
    "            \"net_state_dict\": net.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        }\n",
    "        checkpoint = Checkpoint.from_dict(checkpoint_data)\n",
    "\n",
    "        session.report(\n",
    "            {\"loss\": val_loss/val_steps, \"accuracy\": correct/total},\n",
    "            checkpoint=checkpoint,\n",
    "        )\n",
    "    print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=2):\n",
    "    config = {\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"batch_size\": tune.choice([2**x for x in range(6)]),\n",
    "        \"encoder_depth\": tune.choice(range(3,8)),\n",
    "    }\n",
    "\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2,\n",
    "    )\n",
    "\n",
    "    result = tune.run(\n",
    "        train_unet,\n",
    "        resources_per_trial={\"cpu\": 8, \"gpu\": gpus_per_trial},\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        checkpoint_at_end=True)\n",
    "    \n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(f\"Best trial config: {best_trial.config}\")\n",
    "    print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
    "    print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(num_samples=10, max_num_epochs=10, gpus_per_trial=0)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
