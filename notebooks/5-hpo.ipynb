{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "from config.settings import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kristian/mambaforge/envs/mitotem/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/kristian/mambaforge/envs/mitotem/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms.v2.functional as F\n",
    "import numpy as np\n",
    "from ray import tune\n",
    "from ray.air import Checkpoint, session\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from src.models.unet import UNet, find_next_valid_size\n",
    "from src.data.loaders import load_data_mitosemseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unet(config):\n",
    "    net = UNet(channels_out=config['channels_out'],\n",
    "               encoder_depth=config['encoder_depth'],\n",
    "               dropout=config['dropout'])\n",
    "\n",
    "    device = torch.device('cpu')\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "    common = {k: config[k] for k in ('lr','weight_decay')}\n",
    "    common['params'] = net.parameters()\n",
    "    optims = {\n",
    "        'sgd': optim.SGD(**common, momentum=config['momentum']),\n",
    "        'adam': optim.Adam(**common),\n",
    "        'adamw': optim.AdamW(**common),\n",
    "    }\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "    optimizer = optims[config['optimizer']]\n",
    "\n",
    "    checkpoint = session.get_checkpoint()\n",
    "    if checkpoint:\n",
    "        checkpoint_state = checkpoint.to_dict()\n",
    "        start_epoch = checkpoint_state[\"epoch\"]\n",
    "        net.load_state_dict(checkpoint_state[\"net_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n",
    "    else:\n",
    "        start_epoch = 1\n",
    "\n",
    "    data_dir = os.getenv('LOCAL_SCRATCH') + '/dataset'\n",
    "    input_size, output_size = find_next_valid_size(1000, 3, config['encoder_depth'])\n",
    "    trainset, valset = load_data_mitosemseg(data_dir, input_size, split=0.85)\n",
    "\n",
    "    train_iter = DataLoader(trainset, config['batch_size'], shuffle=True, num_workers=16)\n",
    "    val_iter = DataLoader(valset, config['batch_size'], shuffle=False, num_workers=16)\n",
    "\n",
    "    for epoch in range(start_epoch, 11): # max epochs is 10\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, (inputs, targets, weights) in enumerate(train_iter,1):\n",
    "            targets = F.center_crop(targets, output_size)\n",
    "            weights = F.center_crop(weights, output_size)\n",
    "            inputs, targets, weights = inputs.to(device), targets.to(device), weights.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = ((1+weights)*criterion(outputs, targets)).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps +=1\n",
    "            if i % 32 == 0:\n",
    "                print(f\"[{epoch}, {i:>5}] loss: {running_loss/epoch_steps:.3f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        for i, (inputs, targets) in enumerate(val_iter, 1):\n",
    "            with torch.no_grad():\n",
    "                targets = F.center_crop(targets, output_size)\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        checkpoint_data = {\n",
    "            \"epoch\": epoch,\n",
    "            \"net_state_dict\": net.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        }\n",
    "        checkpoint = Checkpoint.from_dict(checkpoint_data)\n",
    "\n",
    "        session.report({\"loss\": val_loss/val_steps}, checkpoint=checkpoint)\n",
    "                       \n",
    "    print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_size\": tune.grid_search([1,2,4,8]),\n",
    "    \"optimizer\": tune.grid_search(['sgd','adam','adamw']),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"dropout\": tune.uniform(0.0,0.5),\n",
    "    \"momentum\": tune.sample_from(\n",
    "        lambda spec: \n",
    "        np.random.uniform(0.5, 0.99) if spec.config.optimizer == 'sgd' else None),\n",
    "    \"weight_decay\": tune.grid_search([1e-3, 1e-4, 1e-5, 0]),\n",
    "    \"channels_out\": tune.randint(32,65),\n",
    "    \"encoder_depth\": tune.grid_search([3,4,5]),\n",
    "}\n",
    "\n",
    "scheduler = ASHAScheduler(\n",
    "    grace_period=1,\n",
    "    reduction_factor=2,\n",
    ")\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    tune.with_resources(\n",
    "        tune.with_parameters(train_unet),\n",
    "        resources={\"gpu\": 2},\n",
    "    ),\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        scheduler=scheduler,\n",
    "        num_samples=20,\n",
    "    ),\n",
    "    param_space=config,\n",
    ")\n",
    "\n",
    "result = tuner.fit()\n",
    "\n",
    "best_result = result.get_best_result(\"loss\", \"min\", \"last\")\n",
    "print(f\"Best trial config: {best_result.config}\")\n",
    "print(f\"Best trial final validation loss: {best_result.last_result['loss']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mitotem-kernel",
   "language": "python",
   "name": "mitotem-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
