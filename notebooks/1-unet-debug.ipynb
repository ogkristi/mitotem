{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67c295ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, out_channels, num_convs=2, kernel_size=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        for _ in range(num_convs):\n",
    "            layers.append(nn.LazyConv2d(out_channels, kernel_size=kernel_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        self.convblock = nn.Sequential(*layers)\n",
    "        self.poolblock = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        skip = self.convblock(X)\n",
    "        Y = self.poolblock(skip)\n",
    "        return Y, skip\n",
    "        \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, out_channels, num_convs=2, kernel_size=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.unpoolblock = nn.LazyConvTranspose2d(out_channels, kernel_size=2, stride=2)\n",
    "        \n",
    "        layers = []\n",
    "        for _ in range(num_convs):\n",
    "            layers.append(nn.LazyConv2d(out_channels, kernel_size=kernel_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.convblock = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, X, skip):\n",
    "        X_up = self.unpoolblock(X)\n",
    "        m, n = X_up.shape[-2:]\n",
    "        d_m = (skip.shape[-2]-m) // 2\n",
    "        d_n = (skip.shape[-1]-n) // 2\n",
    "        # Center crop skip features to same height/width as X_up\n",
    "        # then concatenate them along the output channel dimension\n",
    "        X_up_cat = torch.cat((X_up,skip[:, :, d_m:m+d_m, d_n:n+d_n]), dim=1)\n",
    "        Y = self.convblock(X_up_cat)\n",
    "        return Y\n",
    "        \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, channels_out, encoder_depth=4, num_classes=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.ModuleList()\n",
    "        for i in range(0,encoder_depth):\n",
    "            self.encoder.append(EncoderBlock(channels_out*2**i))\n",
    "            \n",
    "        self.bridge = nn.Sequential(\n",
    "            nn.LazyConv2d(channels_out*2**encoder_depth, kernel_size=3), nn.ReLU(),\n",
    "            nn.LazyConv2d(channels_out*2**encoder_depth, kernel_size=3), nn.ReLU(),            \n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.ModuleList()\n",
    "        for i in range(encoder_depth-1,-1,-1):\n",
    "            self.decoder.append(DecoderBlock(channels_out*2**i))\n",
    "            \n",
    "        self.final = nn.LazyConv2d(num_classes, kernel_size=1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # H = hidden layer activations\n",
    "        H = X\n",
    "        skip = []\n",
    "        \n",
    "        for enc_block in self.encoder:\n",
    "            H, S = enc_block(H)\n",
    "            skip.append(S)\n",
    "            \n",
    "        H = self.bridge(H)\n",
    "        \n",
    "        for dec_block in self.decoder:\n",
    "            H = dec_block(H, skip.pop())\n",
    "            \n",
    "        Y = self.final(H)\n",
    "        return Y\n",
    "    \n",
    "    def init_weights(self, inputs):\n",
    "        self.forward(inputs)\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a984f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kristian/mambaforge/envs/mitotem/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.1735,  1.3608, -0.5569,  ...,  0.4138, -1.2697,  1.4504],\n",
      "          [ 0.9092,  1.4605, -1.5454,  ...,  0.1866,  0.7423,  0.7390],\n",
      "          [-1.4064,  0.6028, -0.0883,  ...,  0.5749,  0.0245,  0.3559],\n",
      "          ...,\n",
      "          [ 0.2521, -0.0932,  0.7357,  ..., -0.5910,  0.1675, -0.2962],\n",
      "          [-0.3453,  0.7124,  0.6962,  ...,  0.4802, -0.7110,  0.0048],\n",
      "          [ 0.0724, -0.0707,  1.1107,  ...,  0.6663,  1.1248, -0.9702]],\n",
      "\n",
      "         [[ 0.3904, -0.2253, -1.9845,  ..., -2.1738, -0.8560, -0.7408],\n",
      "          [-1.4031, -0.8858, -1.8994,  ...,  0.0403, -1.2398, -0.7838],\n",
      "          [-1.2248, -0.9576, -0.8792,  ..., -0.5345, -0.1558, -1.8179],\n",
      "          ...,\n",
      "          [-1.7406, -0.7554, -0.7882,  ..., -2.5429, -1.0109, -1.6083],\n",
      "          [-0.0935, -0.4155, -0.4309,  ..., -0.7168, -1.9295, -1.0563],\n",
      "          [-1.6012, -0.5487, -0.9493,  ..., -1.1098, -0.9146, -1.9235]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "[ModuleList(\n",
      "  (0): EncoderBlock(\n",
      "    (convblock): Sequential(\n",
      "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (poolblock): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (1): EncoderBlock(\n",
      "    (convblock): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (poolblock): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (2): EncoderBlock(\n",
      "    (convblock): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (poolblock): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (3): EncoderBlock(\n",
      "    (convblock): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (poolblock): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "), Sequential(\n",
      "  (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (3): ReLU()\n",
      "), ModuleList(\n",
      "  (0): DecoderBlock(\n",
      "    (unpoolblock): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (convblock): Sequential(\n",
      "      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): DecoderBlock(\n",
      "    (unpoolblock): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (convblock): Sequential(\n",
      "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (2): DecoderBlock(\n",
      "    (unpoolblock): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (convblock): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (3): DecoderBlock(\n",
      "    (unpoolblock): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (convblock): Sequential(\n",
      "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "), Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))]\n"
     ]
    }
   ],
   "source": [
    "unet = UNet(64)\n",
    "I = torch.randn((1,1,572,572))\n",
    "unet.init_weights(I)\n",
    "print(unet(I))\n",
    "print(list(unet.children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0535a3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mitotem-kernel",
   "language": "python",
   "name": "mitotem-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
