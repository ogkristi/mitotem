{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67c295ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, out_channels, num_convs=2, kernel_size=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        for _ in range(num_convs):\n",
    "            layers.append(nn.LazyConv2d(out_channels, kernel_size=kernel_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        self.convblock = nn.Sequential(*layers)\n",
    "        self.poolblock = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        skip = self.convblock(X)\n",
    "        Y = self.poolblock(skip)\n",
    "        return Y, skip\n",
    "        \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, out_channels, num_convs=2, kernel_size=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.unpoolblock = nn.LazyConvTranspose2d(out_channels, kernel_size=2, stride=2)\n",
    "        \n",
    "        layers = []\n",
    "        for _ in range(num_convs):\n",
    "            layers.append(nn.LazyConv2d(out_channels, kernel_size=kernel_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.convblock = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, X, skip):\n",
    "        X_up = self.unpoolblock(X)\n",
    "        m, n = X_up.shape[-2:]\n",
    "        d_m = (skip.shape[-2]-m) // 2\n",
    "        d_n = (skip.shape[-1]-n) // 2\n",
    "        # Center crop skip features to same height/width as X_up\n",
    "        # then concatenate them along the output channel dimension\n",
    "        X_up_cat = torch.cat((X_up,skip[:, :, d_m:m+d_m, d_n:n+d_n]), dim=1)\n",
    "        Y = self.convblock(X_up_cat)\n",
    "        return Y\n",
    "        \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, channels_out, encoder_depth=4, num_classes=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.ModuleList()\n",
    "        for i in range(0,encoder_depth):\n",
    "            self.encoder.append(EncoderBlock(channels_out*2**i))\n",
    "            \n",
    "        self.bridge = nn.Sequential(\n",
    "            nn.LazyConv2d(channels_out*2**encoder_depth, kernel_size=3), nn.ReLU(),\n",
    "            nn.LazyConv2d(channels_out*2**encoder_depth, kernel_size=3), nn.ReLU(),            \n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.ModuleList()\n",
    "        for i in range(encoder_depth-1,-1,-1):\n",
    "            self.decoder.append(DecoderBlock(channels_out*2**i))\n",
    "            \n",
    "        self.final = nn.LazyConv2d(num_classes, kernel_size=1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # H = hidden layer activations\n",
    "        H = X\n",
    "        skip = []\n",
    "        \n",
    "        for enc_block in self.encoder:\n",
    "            H, S = enc_block(H)\n",
    "            skip.append(S)\n",
    "            \n",
    "        H = self.bridge(H)\n",
    "        \n",
    "        for dec_block in self.decoder:\n",
    "            H = dec_block(H, skip.pop())\n",
    "            \n",
    "        Y = self.final(H)\n",
    "        return Y\n",
    "    \n",
    "    def init_weights(self, inputs):\n",
    "        self.forward(inputs)\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a984f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kristian/.local/lib/python3.11/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.7961e-01,  5.8380e-01,  7.3720e-01,  ...,  8.7482e-01,\n",
      "            1.0781e+00,  1.4031e+00],\n",
      "          [ 1.4914e-01,  1.8723e-01,  2.3184e-01,  ...,  1.2077e+00,\n",
      "            3.9147e-01,  1.7680e-01],\n",
      "          [ 5.6973e-01,  6.5522e-01,  9.0722e-01,  ...,  8.8139e-01,\n",
      "           -7.5916e-03,  6.5021e-01],\n",
      "          ...,\n",
      "          [ 4.8358e-01,  6.8153e-01, -5.7561e-01,  ...,  2.9997e-01,\n",
      "           -1.3754e-01,  5.0778e-02],\n",
      "          [ 6.0585e-01,  3.9180e-01, -1.2003e-01,  ...,  7.4532e-01,\n",
      "            1.0719e-01,  7.9113e-01],\n",
      "          [ 7.2439e-01, -2.2501e-02,  5.5179e-01,  ...,  5.2347e-01,\n",
      "            1.4869e+00,  1.4965e-01]],\n",
      "\n",
      "         [[ 1.4314e+00, -2.7129e-01,  1.7169e+00,  ...,  3.1603e-01,\n",
      "            1.6929e+00,  6.7742e-01],\n",
      "          [ 2.1997e-01, -5.0266e-01, -2.5307e-01,  ...,  1.7772e-01,\n",
      "            9.9475e-01, -4.2054e-01],\n",
      "          [ 8.6702e-01,  4.9469e-01,  1.2595e+00,  ...,  8.2479e-01,\n",
      "            1.5853e+00,  1.4610e+00],\n",
      "          ...,\n",
      "          [-3.3688e-01,  5.9427e-01, -8.1464e-01,  ...,  3.4444e-01,\n",
      "            9.5379e-01, -2.1438e-02],\n",
      "          [ 1.3858e+00,  5.6206e-01,  4.0233e-01,  ...,  8.8865e-01,\n",
      "            6.8523e-01,  1.2724e+00],\n",
      "          [-5.0060e-01,  3.1925e-01, -1.9754e-03,  ...,  1.9972e+00,\n",
      "            4.4303e-01, -1.2491e+00]]]], grad_fn=<ConvolutionBackward0>)\n",
      "[ModuleList(\n",
      "  (0): EncoderBlock(\n",
      "    (convblock): Sequential(\n",
      "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (poolblock): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (1): EncoderBlock(\n",
      "    (convblock): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (poolblock): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (2): EncoderBlock(\n",
      "    (convblock): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (poolblock): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (3): EncoderBlock(\n",
      "    (convblock): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (poolblock): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "), Sequential(\n",
      "  (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (3): ReLU()\n",
      "), ModuleList(\n",
      "  (0): DecoderBlock(\n",
      "    (unpoolblock): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (convblock): Sequential(\n",
      "      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): DecoderBlock(\n",
      "    (unpoolblock): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (convblock): Sequential(\n",
      "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (2): DecoderBlock(\n",
      "    (unpoolblock): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (convblock): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (3): DecoderBlock(\n",
      "    (unpoolblock): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (convblock): Sequential(\n",
      "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "), Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))]\n"
     ]
    }
   ],
   "source": [
    "unet = UNet(64)\n",
    "I = torch.randn((1,1,572,572))\n",
    "unet.init_weights(I)\n",
    "print(unet(I))\n",
    "print(list(unet.children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0535a3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mitotem-kernel",
   "language": "python",
   "name": "mitotem-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
